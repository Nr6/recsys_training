{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit 5: Model-based Collaborative Filtering for **Rating** Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this unit, we change the approach towards CF from neighborhood-based to **model-based**. This means that we create and train a model for describing users and items instead of using the k nearest neighbors. The model parameters are latent representations for users and items.\n",
    "\n",
    "Key to this idea is to compress the sparse interaction information of $R$ by finding two matrices $U$ and $V$ that by multiplication reconstruct $R$. The decomposition of $R$ into $U \\times V$ is called _matrix factorization_ and we refer to $U$ as user latent factor matrix and $V$ as item latent factor matrix.\n",
    "\n",
    "Compressing the sparse matrix into the product of two matrices means that the two remaining matrices are much smaller. This decrease in size is governed by the dimension of latent user/item vectors and symbolized by $d \\in \\mathbb{N}$. We choose $d$ to be much smaller than the number of items or users:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\underset{m\\times n}{\\mathrm{R}} \\approx  \\underset{m\\times d}{U} \\times \\underset{d\\times n}{V^T} \\\\\n",
    "dÂ \\ll \\min\\{m, n\\}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import itertools\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from recsys_training.data import Dataset\n",
    "from recsys_training.evaluation import get_relevant_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml100k_ratings_filepath = '../data/raw/ml-100k/u.data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Dataset(ml100k_ratings_filepath)\n",
    "data.rating_split(seed=42)\n",
    "user_ratings = data.get_user_ratings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the user and item latent factors, i.e. the model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "m = data.n_users\n",
    "n = data.n_items\n",
    "d = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we want to learn the user/item latent factors from rating data, we first randomly initialize them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed)\n",
    "user_factors = np.random.normal(0, 1, (m, d))\n",
    "item_factors = np.random.normal(0, 1, (n, d))\n",
    "ratings = data.train_ratings[['user', 'item', 'rating']].sample(frac=1, random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We fit the model to the data with a technique called _minibatch gradient descent_.\n",
    "\n",
    "This means that for a number of epochs, i.e. full passes through the training data (ratings), we randomly choose a small subset of ratings (our minibatch) holding user, item and rating for each instance. Then, we compute the rating prediction as the dot product of user and item latent vectors (also called embeddings) and compute the mean squared error between predicted and true rating. We derive this error for user and item latent vectors to obtain our partial derivatives. We subtract part of the gradient from our latent vectors to move into the direction of minimizing error, i.e. deviation between true values and predictions.\n",
    "\n",
    "To keep track of the decreasing error, we compute the root mean squared error and print it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "batch_size = 64\n",
    "learning_rate = 0.01\n",
    "\n",
    "num_batches = int(np.ceil(len(ratings) / batch_size))\n",
    "rmse_trace = []\n",
    "rmse_test_trace = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task:** Implement `compute_gradients` that receives a minibatch and computes the gradients for user and item latent vectors involved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradients(ratings: np.array,\n",
    "                      u: np.array,\n",
    "                      v: np.array) -> Tuple[np.array, np.array]:\n",
    "    pass\n",
    "\n",
    "    return u_grad, v_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rmse(rating, u, v) -> float:\n",
    "    pred = np.sum(u * v, axis=1)\n",
    "    error = rating - pred\n",
    "    rmse = np.sqrt(np.mean(error ** 2))\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    for idx in range(num_batches):\n",
    "        \n",
    "        minibatch = ratings.iloc[idx * batch_size:(idx + 1) * batch_size]\n",
    "        \n",
    "        # deduct 1 as user ids are 1-indexed, but array is 0-indexed\n",
    "        user_embeds = user_factors[minibatch['user'].values - 1]\n",
    "        item_embeds = item_factors[minibatch['item'].values - 1]\n",
    "\n",
    "        user_grads, item_grads = compute_gradients(minibatch['rating'].values,\n",
    "                                                   user_embeds,\n",
    "                                                   item_embeds)\n",
    "        \n",
    "        # update user and item factors\n",
    "        user_factors[minibatch['user'].values - 1] -= learning_rate * user_grads\n",
    "        item_factors[minibatch['item'].values - 1] -= learning_rate * item_grads\n",
    "\n",
    "        if not idx % 300:\n",
    "            rmse = get_rmse(minibatch['rating'].values,\n",
    "                            user_embeds,\n",
    "                            item_embeds)\n",
    "            rmse_test = get_rmse(data.test_ratings['rating'].values,\n",
    "                                 user_factors[data.test_ratings['user'].values - 1],\n",
    "                                 item_factors[data.test_ratings['user'].values - 1])\n",
    "            rmse_trace.append(rmse)\n",
    "            rmse_test_trace.append(rmse_test)\n",
    "            print(f\"Epoch: {epoch:02d}, RMSE: {rmse:.3f}, Test RMSE: {rmse_test:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(range(len(rmse_trace)), rmse_trace, 'b--', label='Train')\n",
    "plt.plot(range(len(rmse_test_trace)), rmse_test_trace, 'g--', label='Test')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('RMSE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the model for Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now created a model to describe users and items in terms of latent vectors. We fitted them to reconstruct ratings by multiplication. So for obtaining recommendations we simply multiply user-item latent vectors we are interested in and see favorable combinations where predicted ratings, i.e. the products, are rather high.\n",
    "\n",
    "Thus, before writing the `get_recommendations` we first implement `get_prediction`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task:** Implement `get_prediction` for predicting ratings for a user and all items or a set of provided items. Remember to remove _known positives_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(user,\n",
    "                   items: np.array = None,\n",
    "                   remove_known_pos: bool = True) -> Dict[int, Dict[str, float]]:\n",
    "    pass\n",
    "\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_predictions = get_prediction(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(item_predictions.items())[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recommendations(user: int, N: int, remove_known_pos: bool = False) -> List[Tuple[int, Dict[str, float]]]:\n",
    "    predictions = get_prediction(user, remove_known_pos=remove_known_pos)\n",
    "    recommendations = []\n",
    "    for item, pred in predictions.items():\n",
    "        add_item = (item, pred)\n",
    "        recommendations.append(add_item)\n",
    "        if len(recommendations) == N:\n",
    "            break\n",
    "\n",
    "    return recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendations = get_recommendations(1, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_items = get_relevant_items(data.test_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = relevant_items.keys()\n",
    "prec_at_N = dict.fromkeys(data.users)\n",
    "\n",
    "for user in users:\n",
    "    recommendations = get_recommendations(user, N, remove_known_pos=True)\n",
    "    recommendations = [val[0] for val in recommendations]\n",
    "    hits = np.intersect1d(recommendations,\n",
    "                          relevant_items[user])\n",
    "    prec_at_N[user] = len(hits)/N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean([val for val in prec_at_N.values() if val is not None])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
